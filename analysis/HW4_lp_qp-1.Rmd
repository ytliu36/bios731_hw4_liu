---
title: "Homework 4"
header-includes: \usepackage{multirow}
output:
  pdf_document:
      latex_engine: xelatex
  html_document:
    df_print: paged
urlcolor: blue
---

```{r, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(tidy = FALSE)
library(here)
library(corrplot)
```

## Context

This assignment reinforces ideas in Module 4: Constrained Optimization. We focus specifically on implementing quantile regression and LASSO.


## Due date and submission

Please submit (via Canvas) a PDF containing a link to the web address of the GitHub repo containing your work for this assignment; git commits after the due date will cause the assignment to be considered late. Due date is Wednesday, 4/2 at 10:00AM.



## Points

```{r, echo = FALSE}
tibble(
  Problem = c("Problem 0", "Problem 1", "Problem 2", "Problem 3"),
  Points = c(20, 20, 30, 30)
) %>%
  knitr::kable()
```

## Dataset 

The dataset for this homework assignment is in the file `cannabis.rds`. It comes from a study conducted by researchers at the University of Colorado who are working to develop roadside tests for detecting driving impairment due to cannabis use.  In this study, researchers measured levels of THC—the main psychoactive ingredient in cannabis—in participants’ blood and then collected other biomarkers and had them complete a series of neurocognitive tests. The goal of the study is to understand the relationship between performance on these neurocognitive tests and the concentration of THC metabolites in the blood. 

The dataset contains the following variables:

* `id`: subject id
* `t_mmr1`: Metabolite molar ratio—a measure of THC metabolites in the blood. This is the outcome variable.
* `p_*`: variables with the `p_` prefix contain measurements related to pupil response to light. 
* `i_*`: variables with the `i_` prefix were collected using an iPad and are derived from neurocognitive tests assessing reaction time, judgment, and short-term memory.
* `h_*`: Variables related to heart rate and blood pressure.



## Problem 0 

This "problem" focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files.

To that end:

* Create a public GitHub repo + local R Project; I suggest naming this repo / directory bios731_hw4_YourLastName (e.g. bios731_hw4_wrobel for Julia)
* Submit your whole project folder to GitHub 
* Submit a PDF knitted from Rmd to Canvas. Your solutions to the problems here should be implemented in your .Rmd file, and your git commit history should reflect the process you used to solve these Problems.

**Github repo:** https://github.com/ytliu36/bios731_hw4_liu.git

## Problem 1: Exploratory data analysis

Perform some EDA for this data. Your EDA should explore the following questions:

- What are $n$ and $p$ for this data?
- What is the distribution of the outcome?
- How correlated are variables in the dataset?

Summarize key findings from your EDA in one paragraph and 2-3 figures or tables. 

```{r, echo = F}
dat<-readRDS(here("data","cannabis.rds"))
n<-length(unique(dat$id))
p<-ncol(dat)-2 #remove 1 id and 1 outcome
# Compute correlation matrix
hist(dat$t_mmr1, main = "Distribution of Metabolite Molar Ratio", xlab = "mmr")
```

```{r, echo = F, fig.width=9, fig.height=7}
# Compute correlation matrix
cor_matrix <- cor(dat[,-c(1,2)], use = "pairwise.complete.obs") 
corrplot(cor_matrix, type = "lower", tl.col = "black")
```

**Summary:** There are 57 subjects and 27 predictors in the dataset. The outcome, metabolite molar ratio is right-skewed. Based on the corrlation matrix, there's not many highly correlated predictors, only the correlation between 8 pairs, including: p_fpc1 and p_change (r = 0.72), p_change and p_auc (r = -0.76), i_prop_false_timeout and i_prop_failed2 (r = 0.64), i_judgement_time1 and i_judgement_time2 (r = 0.74), i_rep_shapes34 and i_memory_time34 (r = -0.62),  i_rep_shapes34 and i_composite_score (r = -0.90), i_memory_time12 and i_memory_time34 (r = 0.66), h_dbp and h_sbp (r = 0.72).

## Problem 2: Quantile regression

Use linear programming to estimate the coefficients for a quantile regression. You need to write a
function named `my_rq`, which takes a response vector $y$, a covariate matrix $X$ and quantile $\tau$ , and
returns the estimated coefficients. Existing linear programming functions can be used directly to
solve the LP problem (for example, `simplex` function in the `boot` package, or `lp` function in the `lpSolve`
package). 

* Use your function to model `t_mmr1` from the cannabis data using `p_change` (percent change in pupil diameter in response to light), `h_hr` (heart rate), and `i_composite_score` (a composite score of the ipad variables) as variables.
* Compare your results with though estimated using the `rq` function in R at quantiles $\tau \in \{0.25, 0.5, 0.75\}$.
* Compare with mean obtain using linear regression
* Summarize findings

```{r}
library(lpSolve)

my_rq <- function(y, X, tau) {
  n <- length(y)
  p <- ncol(X)
  
  # Modify design matrix to allow negative betas
  X_new <- cbind(X, -X)  # Create β+ and β- (splitting each β into positive and negative parts)
  
  # Construct LP problem
  f.obj <- c(rep(0, 2 * p), rep(tau, n), rep(1 - tau, n))  # Objective function
  f.con <- cbind(X_new, diag(n), -diag(n))  # Constraint matrix
  f.rhs <- y  # Right-hand side
  f.dir <- rep("=", n)  # Constraints are equality constraints
  
  # Solve LP
  lp_result <- lp("min", f.obj, f.con, f.dir, f.rhs, all.int = FALSE)
  
  # Extract beta coefficients
  beta_hat <- lp_result$solution[1:p] - lp_result$solution[(p + 1):(2 * p)]  # β = β+ - β-
  return(beta_hat)
}


```

```{r}
library(quantreg)

# Prepare variables
y <- dat$t_mmr1
X <- cbind(1, dat$p_change, dat$h_hr, dat$i_composite_score)  # Add intercept

# Estimate at quantiles 0.25, 0.5, and 0.75
tau_vals <- c(0.25, 0.5, 0.75)
my_rq_results <- lapply(tau_vals, function(tau) my_rq(y, X, tau))
names(my_rq_results) <- paste0("tau_", tau_vals)

# Compare with rq()
rq_results <- lapply(tau_vals, function(tau) coef(rq(t_mmr1 ~ p_change + h_hr + i_composite_score, tau = tau, data = dat)))
names(rq_results) <- paste0("tau_", tau_vals)

# Compare with OLS
ols_result <- coef(lm(t_mmr1 ~ p_change + h_hr + i_composite_score, data = dat))

# Print results
list("My Quantile Regression" = my_rq_results, "RQ Function" = rq_results, "OLS" = ols_result)

```

When explaining your results, be sure to explain what LP method you used for estimating quantile regression.

**Answer:** 
As for the quantile regression, it minimize $\sum_{i=1}^n \rho_\tau(y_i-x_i\beta)$, which is equivalent to minimize $\sum_{i=1}^n(\tau u_i+(1-\tau)v_i)$ when define $u_i = [y_i-x_i\beta]_+ \geq0$ and $v_i = [y_i-x_i\beta]_-\geq0$ and $y_i = x_i\beta+u_i-v_i$. The above linear programming is solved by `lp()` in `lpSolve`.

`my_rq()` and `rq()` give the same result at each quantile given, and the association between predictors and outcomes vary at each quantile, p_change and t_mmr1 is positively correlated, but with highest correlation at $\tau$ = 0.5 compared to the other 2. h_hr is also positively correlated with t_mmr1 and correlation become stronger with higher $\tau$. At smaller $\tau$, i_composite_score has positive correlation at $\tau$ = 0.25 and 0.5, but become negative when $\tau$=0.75. OLS is able to detect the positive correlation of p_change and h_hr, but failed to identify the positive correlation of i_composite_score at lower quantiles. 

## Problem 3: Implementation of LASSO


As illustrated in class, a LASSO problem can be rewritten as a quadratic programming problem.

1. Many widely used QP solvers require that the matrix in the quadratic function for the second
order term to be positive definite (such as `solve.QP` in the `quadprog` package). Rewrite the
quadratic programming problem for LASSO in matrix form and show that the matrix is not
positive definite, thus QP solvers like `solve.QP` cannot be used. 
2. The `LowRankQP` function in the `LowRankQP` package can handle the non positive definite situation. Use the
matrix format you derived above and `LowRankQP` to write your own function `my_lasso()` to
estimate the coefficients for a LASSO problem. Your function needs to take three parameters:
$Y$ (response), $X$ (predictor), and $lambda$ (tuning parameter), and return the estimated coefficients.


* Use your function to model `log(t_mmr1)` from the cannabis data using all other variables as potential covariates in the model
* Compare your results with those estimated using the `cv.glmnet` function in R from the `glmnet` package
* Summarize findings

```{r}
library(LowRankQP)

my_lasso <- function(Y, X, lambda) {
  n <- nrow(X)
  p <- ncol(X)
  
  mat<-cbind(diag(p), -diag(p))
  # Construct quadratic term
  Dmat <- t(mat)%*%crossprod(X)%*%mat # X'X (2p x 2p)-alpha estimated is c(u,v)
  dvec <- -t(mat)%*%crossprod(X, Y) # -X'Y (2p x 1)
  
  # Constraints:sum u+v = lambda
  Amat <- matrix(rep(1, 2 * p), ncol = 2*p)  # (2p x 1)
  bvec <-c(lambda)  # (2p x 1)

  # Solve QP problem
  sol <- LowRankQP(Dmat, dvec, Amat, bvec, u = rep(100, 2*p))

  # Extract beta coefficients
  beta_hat <- sol$alpha[1:p]-sol$alpha[(p+1):(2*p)]
  return(beta_hat)
}
```

```{r}
library(glmnet)
# Prepare X and Y
X <- as.matrix(dat[, -c(1, 2)])
X <- scale(X) 
Y <- log(dat$t_mmr1+1e-4)  # Log-transform outcome

# Run my LASSO function
lambda_val <- 0.7
beta_lasso <- my_lasso(Y, X, lambda_val)
beta_lasso
# Compare with glmnet
cv_fit <- cv.glmnet(X, Y)
beta_glmnet <- coef(cv_fit, s = "lambda.min")[-1]
beta_glmnet
```
```{r}
beta_matrix <- rbind(beta_lasso, beta_glmnet)

barplot(beta_matrix, beside = TRUE, names.arg = colnames(X),
        col = c("blue", "red"), legend.text = c("my_lasso", "cv.glmnet"),
        main = "Coeffecients estimation")

```

The results will not be exactly the same because the estimation procedures are different, but trends (which variables are selected) should be similar.

**Summary:** From the plot above we can see at lambda = 0.7, although the absolute value of my_lasso estimation is relatively small, the predictors selected and the sign of estimated coefficients are mostly the same.